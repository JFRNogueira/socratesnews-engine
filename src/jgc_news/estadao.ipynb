{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estadao:\n",
    "\n",
    "    def __init__(self, base_url='https://www.estadao.com.br/'):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def yesterday_url_str(self):\n",
    "        yesterday = datetime.now() - timedelta(1)\n",
    "        yesterday_str = yesterday.strftime(\"%d/%m/%Y\")\n",
    "        return f'/noticia{yesterday_str}'\n",
    "\n",
    "    def getUrls(self, url):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            \n",
    "            page_source = driver.page_source\n",
    "            driver.quit()\n",
    "            \n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            all_hrefs = soup.find_all('a', class_='image') + soup.find_all('a', class_='news-link')\n",
    "            urls = []\n",
    "            for a in all_hrefs:\n",
    "                href = a.get('href')\n",
    "                if href:\n",
    "                    urls.append(href)\n",
    "            \n",
    "            return list(set(urls)) \n",
    "        except:\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "    def getNewsData(self, url, only_yesterday=True):\n",
    "        try:\n",
    "            browsers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome / 86.0.4240.198Safari / 537.36\"}\n",
    "            page = requests.get(url, headers = browsers)\n",
    "            resposta = page.text\n",
    "            soup = BeautifulSoup(resposta, 'html.parser')\n",
    "\n",
    "            title = soup.find('h1').get_text()\n",
    "            timestamp = soup.find('time').get_text()\n",
    "            isYesterday = self.yesterday_url_str() in timestamp\n",
    "            \n",
    "            content = \"\" \n",
    "            newsBody = soup.find('div', class_=\"news-body\")\n",
    "            for p in newsBody.find_all('p'):\n",
    "                content += p.get_text()\n",
    "                content += \"\\n\"\n",
    "            \n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            if only_yesterday and not isYesterday:\n",
    "                return None\n",
    "\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": content\n",
    "            }\n",
    "        except:\n",
    "            return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
